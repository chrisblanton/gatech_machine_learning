{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction \n",
    "\n",
    "*\\[Machine Learning is a\\] field of study that gives computers the ability to learn without being explicitly programmed.* --- Arthur Samuel, 1959\n",
    "\n",
    "Machine learning has existed for decades; however, until recently, computing power and data storage were too limited to allow machines to solve many problems in the field effectively.\n",
    "The amount of data that is being generated is growing at massive amounts. Humans are not predisposed to be as reliable and efficient as computers at handling large amounts of data.\n",
    "\n",
    "Machine learning is allowing the development of a model where the data and expected output enable the machine to develop a model without a person giving all the details.\n",
    "<img src=\"images/traditional_vs_ml.png\" width=400>\n",
    "\n",
    "\n",
    "Let us compare the traditional approach to solving a problem such as *classifying email as spam or non-spam.* \n",
    "\n",
    "## Programmatic approach\n",
    "\n",
    "Here is an example of the path we might take to develop this classification:\n",
    "\n",
    "1. First, we would identify some common phrases in a spam message (\"4U\", \"credit card\", \"free\", \"amazing\", \"one simple trick\", ...). We might also identify some other common patterns in sender's name, email's name, domains, ...\n",
    "2. We would write set of rules (pattern recognition).\n",
    "3. We would then test our program and likely write more rules\n",
    "4. This would continue until our program worked well enough.\n",
    "\n",
    "## ML Approach\n",
    "\n",
    "1. We take in a large number of emails that have been sorted as *spam* or *not spam*.\n",
    "2. We define some measurable charateristics of the messages, such as domains, message length, ..\n",
    "3. We process the emails through a process of the model determining parameters to give the answer.\n",
    "4. We allow the model to reach an acceptable lvel of accuracy. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practical\n",
    "\n",
    "The questions that arise when embarking on developing a machine learning-based problem come down to obtaining data, put that data in a form that amendable to your system, and developing a modeling. In this workshop, we will go through this for a particular problem. \n",
    "\n",
    "The problem we will targeting in this example will be the detection of breast cancer. Breast cancer is one of the most common cancers among women worldwide. Early diagnosis of breast cancer can greatly decrease the likelihood of death. However, accurate diagnosis can be a challenge and requires expert analysis, which greatly impacts areas where these experts are in short supply. \n",
    "\n",
    "It would be very beneficial to use machine learning techniques to develop an accurate model that can bring expert levels of accuracy to anywhere the models can be used. \n",
    "\n",
    "In this workshop, we will use the Wisconsin breast cancer dataset, a labeled dataset containing 30 parameters obtained by analysis of fine needle aspirate (FNA) of breast masses.\n",
    "This dataset has been previously studied by several papers including [Breast Cancer Detection with Reduced Feature Set](https://www.hindawi.com/journals/cmmm/2015/265138/)\n",
    "\n",
    "\n",
    "Here is a diagram of our plan:\n",
    "<img src=\"images/cancer_diagram.png\">\n",
    "\n",
    "## Steps in this project\n",
    "\n",
    "1. Getting the needed libraries.\n",
    "2. Getting the data\n",
    "3. Getting the data into a usable form.\n",
    "4. Exploring the data.\n",
    "5. Preparing the data for training.\n",
    "6. Preparing the training and testing sets\n",
    "7. Training the model.\n",
    "8. Testing the model.\n",
    "\n",
    "## Libraries\n",
    "\n",
    "Here are the basic libaries that we will use in this notebook for our machine learning workshop. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "import sklearn\n",
    "%matplotlib inline\n",
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us check some information about one of the packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"TensorFlow version: {}\".format(tf.__version__))\n",
    "print(\"Eager execution is: {}\".format(tf.executing_eagerly()))\n",
    "print(\"Keras version: {}\".format(tf.keras.__version__))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Breast Cancer data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer # Loading the breast cancer from a standard datasource within SciKit\n",
    "cancer = load_breast_cancer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's look at the data we have:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cancer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While that is somewhat human-readable, so let's start to transform it. \n",
    "\n",
    "Let's start by getting the name of the fields."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cancer.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can start to see get some information by looking at one of the components, which contains a description for users of the data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cancer['DESCR'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will look at the labels (ML-terminology) for the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cancer['target'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the labels in a code-format. We can see what the meanings of the codes by looking at the 'target_names' field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cancer['target_names'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We note that the array is 0-indexed, so that 0 is 'malignant' and 1 is 'benign'. \n",
    "\n",
    "The features of the set can be found by looking at the 'features_names'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cancer['feature_names'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DataFrame\n",
    "\n",
    "We will use a Pandas DataFrame object to make maniplulation of the data easier. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cancer = pd.DataFrame(np.c_[cancer['data'],cancer['target']],columns=np.append(cancer['feature_names'],['target']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An issue to be aware of is that TensorFlow does not allow spaces in Feature names, so we will fix that now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in df_cancer.keys():\n",
    "    newkey = key.replace(\" \", \"_\")\n",
    "    df_cancer.rename(index=str,columns={key:newkey},inplace=True)\n",
    "print(df_cancer.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cancer.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_cancer.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Scaling\n",
    "\n",
    "We can note that there is a large difference in the magnitutes of the features. In order to avoid any issues that this may cause, we can scale the data to a uniform range. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "df_scaled = df_cancer.copy()\n",
    "df_scaled[['mean_radius', 'mean_texture', 'mean_perimeter', 'mean_area',\n",
    "       'mean_smoothness', 'mean_compactness', 'mean_concavity',\n",
    "       'mean_concave_points', 'mean_symmetry', 'mean_fractal_dimension',\n",
    "       'radius_error', 'texture_error', 'perimeter_error', 'area_error',\n",
    "       'smoothness_error', 'compactness_error', 'concavity_error',\n",
    "       'concave_points_error', 'symmetry_error', 'fractal_dimension_error',\n",
    "       'worst_radius', 'worst_texture', 'worst_perimeter', 'worst_area',\n",
    "       'worst_smoothness', 'worst_compactness', 'worst_concavity',\n",
    "       'worst_concave_points', 'worst_symmetry', 'worst_fractal_dimension']]=  scaler.fit_transform(df_scaled[['mean_radius', 'mean_texture', 'mean_perimeter', 'mean_area',\n",
    "       'mean_smoothness', 'mean_compactness', 'mean_concavity',\n",
    "       'mean_concave_points', 'mean_symmetry', 'mean_fractal_dimension',\n",
    "       'radius_error', 'texture_error', 'perimeter_error', 'area_error',\n",
    "       'smoothness_error', 'compactness_error', 'concavity_error',\n",
    "       'concave_points_error', 'symmetry_error', 'fractal_dimension_error',\n",
    "       'worst_radius', 'worst_texture', 'worst_perimeter', 'worst_area',\n",
    "       'worst_smoothness', 'worst_compactness', 'worst_concavity',\n",
    "       'worst_concave_points', 'worst_symmetry', 'worst_fractal_dimension']])\n",
    "print(df_scaled.head())\n",
    "print(df_scaled.tail())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intelligent selection of Data \n",
    "\n",
    "Despite the difficulity that arises in the obtaining of data, we often have redudant data and too much data for an efficient model. In our case, we can think about how we know we have multiple descriptions of the same property (*mean* and *worst*). In addition, how can we make selections if we wish to reduce our variables without using our own biases? \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_scaled.corr()['target'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization of the Data\n",
    "\n",
    "Let's look at the relaitonships between some of the variables, to get some idea of which data are important and which data are redundant. \n",
    "\n",
    "We'll use a heatmap, which can be used to visualize correlations. Each square of this heatmap shows the correlation between the variables. Correlation close to -1 shows a strong negative correlation (one variable increases as one decreases) and correlation close to +1 shows a strong postive correlation (both increase together). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(df_scaled.corr(),annot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, the default size can be difficult to view. Let's try to make that a little better. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,10))\n",
    "ax = sns.heatmap(df_scaled.corr(),annot=True) # This is because of an issue in matplotlib. \n",
    "bottom, top = ax.get_ylim() \n",
    "ax.set_ylim(bottom+0.5, top-0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chosing features to be studied\n",
    "\n",
    "It is tempting to use all the features in a haphazard way. In our case, we see several parameters, such as the size of the cells to have multiple types of measurements. It is often advanatageous to try to minimize the number of features because of this and computational expense. \n",
    "\n",
    "Thinking about this in a mathematical sense, the features do not necessarily form a orthogonal basis set. This can lead to degenerate answers which may complicate the optimization process and either lead to a local extrema or failure of convergence. **This is in general terms of optimization, not strictly ML terms.** \n",
    "\n",
    "As related and practical matter, the large the feature set, the more expensive the calcuation is. By reducing the number of features, we try to increase the \"siginal-to-noise\" while decrease the computational expense. \n",
    "\n",
    "In our case, we will use the mean parameters for a starting point because it reduces the number of features to 5. Inuitively, mean values tend to be a good choice for measuring trends.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sns.pairplot(df_scaled, vars=['mean_radius','mean_texture','mean_perimeter','mean_area','mean_smoothness'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Labeling of the data will begin to give some idea of trends within the data sets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.pairplot(df_scaled,hue='target', vars=['mean_radius','mean_texture','mean_perimeter','mean_area','mean_smoothness'])\n",
    "# Below is to allow the legend to use words instead of numbers. \n",
    "handles = g._legend_data.values()\n",
    "labels = ['Malignant','Benign'] \n",
    "g._legend.remove()\n",
    "g.fig.legend(handles=handles,labels=labels, loc='center right',ncol=1)\n",
    "g.fig.subplots_adjust(top=0.92,bottom=0.08,right=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(df_scaled['target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scaled.corr()['target'].sort_values()b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see the redudance in multiple manners and I choose to use the mean values for my model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features=['mean_radius','mean_texture','mean_perimeter','mean_area','mean_smoothness']\n",
    "labels=['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "randomized_data = df_scaled.reindex(np.random.permutation(df_scaled.index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "randomized_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_records = len(randomized_data)\n",
    "training_set_size_portion = 0.8\n",
    "training_set_size = int(total_records*training_set_size_portion)\n",
    "test_set_size = total_records - training_set_size\n",
    "print(total_records,training_set_size,test_set_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building the testing features and labels\n",
    "testing_features = randomized_data.tail(test_set_size)[features].copy()\n",
    "testing_labels = randomized_data.tail(test_set_size)[labels].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_labels.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_features = randomized_data.head(training_set_size)[features].copy()\n",
    "training_labels = randomized_data.head(training_set_size)[labels].copy()\n",
    "print(training_features.head())\n",
    "print(training_labels.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_columns = [tf.feature_column.numeric_column(key) for key in features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(feature_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Choice of Model\n",
    "\n",
    "There are many models that can be used to attempt to solve the problem of classifying wheter the cancer is benign or malignant. In this example, we will use a neural network; which is a mathematical model that is inspired by how brains use. \n",
    "\n",
    "The strength of neural networks has been shown in the ability of these algorithms to excel in certain problems, especially classification. In the case of this problem, there is a deep pattern that is inside the set of data and the cancer outcome (otherwise, how would the physician's determination be better than a random determination). It seems like a fruitiful approach to develop neural network to classify each patient's data in terms of malignant or beign.  \n",
    "\n",
    "#  Neural Networks\n",
    "\n",
    "Neural networks are a type of machine learning algorithm that are inspired by neurons in the human brain. Similar to neurons in the brains, neural networks are formed by interconnecting neurons that interact with each other. Each neuron takes input, does some simple alogrithm to it, and then passes an output to the next neuron. \n",
    "\n",
    "Let us look at a *perceptron*; that is, a *single layer neural network*. \n",
    "\n",
    "<img src='images/perceptron.png'>\n",
    "\n",
    "The *perceptron* is a mathematical function that takes a set of inputs, performs some operation, and outputs the result. In this case,\n",
    "$$ y = \\sum_{i} w_{i}x{i} + w_0,$$\n",
    "where $w_i$ is the weight of the perceptron and $w_0$ is the bias. Note that this is the form of a line (plane,hyperplane,...) The weights are used to determine the importance of the of that component and the bias shifts the activation function curve up and down. \n",
    "\n",
    "The results of the perceptron acting on the inputs, will be input into the activation function, which will determine how to classify the set. \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Architecture of neural networks\n",
    "\n",
    "A neural network consists of \n",
    "* An input layer \n",
    "* Any number of hidden layers (these are called hidden because the external observe does not see the output)\n",
    "* An output layer\n",
    "* A set of weights and bias between each layer $\\{w_i\\}, \\{b_i\\}$\n",
    "* An activation function for each layer, $\\sigma$\n",
    "\n",
    "\n",
    "<img src='images/neural_network_1.png'>\n",
    "\n",
    "## Training Process\n",
    "\n",
    "Each iteration of the training process consists of the following steps:\n",
    "1. Calculating the predicted output $\\hat{y}$, known as _*Feedforward*_\n",
    "2. Updating the weights and biases, known as _*Backpropagation*_\n",
    "\n",
    "Schematicially, this can be illustated as \n",
    "<img src='images/nn_iteration.png'>\n",
    "\n",
    "### Feedforward\n",
    "\n",
    "The forward motion is quite simply the calculation of the function in series, that is the the sum of the products of the weights and activations that lead to the neuron. Swe are moving forward in the network. \n",
    "\n",
    "The loss function comes into play at this point, since we must determine the \"goodness\" of our performance.\n",
    "There are many possibilities to use for the *loss* function, such as the familar *sum-of-squares error*\n",
    "$$ \\mathrm{loss} = \\sum_{i=1}^n (y-\\hat{y})^2$$\n",
    "\n",
    "### Backprogagation \n",
    "\n",
    "As we measure the error of our prediction, we can now find a way to use the error to improve the network, if desired. This is termed *backpropagation*. We work away back to update the weights and biases for the neurons. \n",
    "\n",
    "Minimization of the error function is how this optimization. There are multiple methods to optimize these multiple dimension functions, a popular one method may be to use the derviative of the loss function to determine the path of greatest decrease as in *gradient descent*.\n",
    "\n",
    "## Hyperparameters\n",
    "\n",
    "*Hyperparameters* are the *variables which determine the network structure* and *how the network is trained*. Examples that effect the *learning rate* are *epoch*, *batches*, and *iterations*. These are important parameters that are not learned by the network so they must be specified by the model designer. \n",
    "\n",
    "An *epoch* is when an entire training dataset is passed forward and backward through the network *once*. It is at the end of an epoch that parameters (weights and biases) have updated. In short (batch_size * number_iterations >= number_data)\n",
    "\n",
    "An *iteration* is the number of *batches* needed to complete one epoch.\n",
    "\n",
    "In some cases, the dataset will need to be divided into *batches* in order to fit everything in memory in order complete the calculations. For example, you may have 1000 training examples, \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = tf.estimator.DNNClassifier(feature_columns=feature_columns,hidden_units=[10,10,10], n_classes=2,model_dir='tmp/model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the Network\n",
    "\n",
    "We define the training the input function now. \n",
    "\n",
    "The function that does this is \n",
    "\n",
    "`train_input_fn = tf.estimator.inputs.pandas_input_fn(x=training_features, y=training_labels['target'], num_epochs=15,shuffle=True)`\n",
    "\n",
    "In this case, we will pass through the data set 15 times, updating the weight and biases based on the loss.\n",
    "<https://www.tensorflow.org/api_docs/python/tf/estimator/inputs/pandas_input_fn> for complete documentation of the function.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_input_fn =  tf.compat.v1.estimator.inputs.pandas_input_fn(x=training_features,y=training_labels['target'],num_epochs=15,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(training_features['mean_radius']), type(training_labels['target']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note** If you are reruning the calculation, it may be necessary to clean out the tmp directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.train(input_fn=train_input_fn,steps=2000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_input_fn = tf.compat.v1.estimator.inputs.pandas_input_fn(x=testing_features,y=testing_labels['target'],num_epochs=15,shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.evaluate(input_fn=test_input_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score=classifier.evaluate(input_fn=test_input_fn)['accuracy']\n",
    "print(\"Accuracy = {}\".format(accuracy_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Improving the accuracy\n",
    "\n",
    "Since the accuracy is not very high, let us start to try to improve the accuracy.\n",
    "There are a number of ways to increase accuracy:\n",
    "\n",
    "- Increase hidden layers\n",
    "- Change activiation function\n",
    "- Change activation function in output layer\n",
    "- Increase number of neurons\n",
    "- Weight initialization\n",
    "- More data\n",
    "- Normalization/scaling data\n",
    "- Change learning algorith parameters\n",
    "\n",
    "\n",
    "Let's try to improve the accuracy by increasing the number of hidden layers. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_v2 = tf.estimator.DNNClassifier(feature_columns=feature_columns,hidden_units=[10,20,20,10], n_classes=2,model_dir='tmp/model_2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_v2.train(input_fn=train_input_fn,steps=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "classifier_v2.evaluate(input_fn=test_input_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We have improve the accuracy to 0.86 from 0.36** Let's try to change the activation function. By default, we are using the relu (rectified linear unit). The other options can be found in found in [tf.nn documentation](https://www.tensorflow.org/api_docs/python/tf/nn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%rm -rf './tmp/model_3'\n",
    "classifier_v3 = tf.estimator.DNNClassifier(feature_columns=feature_columns,hidden_units=[10,20,20,10], activation_fn=tf.nn.selu, n_classes=2,model_dir='tmp/model_3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_v3.train(input_fn=train_input_fn,steps=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_v3.evaluate(input_fn=test_input_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We have decrease the accuracy with that.** Let's try something else. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%rm -rf './tmp/model_4'\n",
    "classifier_v4 = tf.estimator.DNNClassifier(feature_columns=feature_columns,hidden_units=[10,20,20,10], n_classes=2,model_dir='tmp/model_4')\n",
    "classifier_v4.train(input_fn=train_input_fn,steps=2000)\n",
    "classifier_v4.evaluate(input_fn=test_input_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feedback \n",
    "\n",
    "<https://gatech.co1.qualtrics.com/jfe/form/SV_55uzMYLufTuiLch>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python:mlws",
   "language": "python",
   "name": "mlws"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
